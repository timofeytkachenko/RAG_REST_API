{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44f9177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = dotenv_values(\".env\")[\"API_KEY\"]\n",
    "api_base = dotenv_values(\".env\")[\"API_BASE_URL\"]\n",
    "model_name = dotenv_values(\".env\")[\"MODEL_NAME\"]\n",
    "client_secret = dotenv_values(\".env\")[\"PROJECT_SERVICE_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e241141",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"api_key\": api_key,\n",
    "    \"base_url\": api_base,\n",
    "    \"default_headers\": {\"Grace-Client-Secret\": client_secret},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81942bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncOpenAI(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b6aeee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "async def qwen_chat(\n",
    "    prompt: str,\n",
    "    system_prompt: Optional[str] = None,\n",
    "    model: Optional[str] = None,\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 0.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Ask Qwen via the existing AsyncOpenAI client and return the text response.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        User prompt to send to the model.\n",
    "    system_prompt : str, optional\n",
    "        Optional system prompt to set assistant behavior.\n",
    "    model : str, optional\n",
    "        Model name to use; defaults to `MODEL_NAME` from .env if not provided.\n",
    "    max_tokens : int, default=512\n",
    "        Maximum tokens to generate.\n",
    "    temperature : float, default=0.0\n",
    "        Sampling temperature for output randomness.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The assistant's text response (empty string if the model returns no content).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # await qwen_chat(\"What is the capital of France?\")  # doctest: +SKIP\n",
    "    >>> # await qwen_chat(\"...\", system_prompt=rag_system_prompt)  # doctest: +SKIP\n",
    "    \"\"\"\n",
    "    try:\n",
    "        selected_model = model or model_name\n",
    "        if not selected_model:\n",
    "            raise ValueError(\n",
    "                \"Model name is not configured. Set MODEL_NAME in .env or pass `model`.\"\n",
    "            )\n",
    "\n",
    "        messages = []\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "        messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "        response = await client.chat.completions.create(\n",
    "            model=selected_model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return content or \"\"\n",
    "    except Exception as exc:\n",
    "        raise RuntimeError(f\"Qwen chat request failed: {exc}\") from exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eceb1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt from main.py RAG service\n",
    "rag_system_prompt = \"\"\"\n",
    "You act as a proctor AI embedded in a Retrieval-Augmented Generation (RAG) framework designed to assist examinees during exams. Your responsibilities include:\n",
    "- Confirming that each question pertains directly to the exam or the specific documents retrieved for that question.\n",
    "- When questions are confirmed exam-related, provide comprehensive, transparent, and courteous answers strictly based on the retrieved document content, integrating relevant details precisely.\n",
    "- If the question lacks supporting context or is unclear, inform the user politely: \"The system cannot answer this question based on current information. Please wait for a real proctor to respond in chat.\"\n",
    "- For questions unrelated to the exam or beyond the document scope, reply with: \"I have no information about the topic you asked. Kindly direct your queries to the proctor in the chat.\"\n",
    "\n",
    "Always maintain professionalism and avoid any information or assumptions outside the retrieved content.\n",
    "\n",
    "Remember, your role is to ensure that examinees receive reliable, context-aware assistance while clearly signalling when human intervention is necessary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "886f09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using normalized OpenAI base_url: https://models.grace-qc.prometric.com/model/candidatemonitor/fastapi/ppllm/model/v1\n"
     ]
    }
   ],
   "source": [
    "from typing import Final\n",
    "\n",
    "\n",
    "def _normalize_openai_base_url(base_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize provider base URL for OpenAI-compatible clients by appending '/v1'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_url : str\n",
    "        Provider base URL from .env (may or may not already include '/v1').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Normalized base URL ending with '/v1' and no trailing slash duplication.\n",
    "    \"\"\"\n",
    "    base = base_url.rstrip(\"/\")\n",
    "    return base if base.endswith(\"/v1\") else base + \"/v1\"\n",
    "\n",
    "\n",
    "# Recreate client with normalized base URL\n",
    "api_base: Final[str] = _normalize_openai_base_url(api_base)\n",
    "config[\"base_url\"] = api_base\n",
    "client = AsyncOpenAI(**config)\n",
    "print(f\"Using normalized OpenAI base_url: {api_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20bf359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model reply:\n",
      " The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Retry simple call after base_url normalization\n",
    "try:\n",
    "    reply = await qwen_chat(\n",
    "        \"What is the capital of France?\", system_prompt=rag_system_prompt\n",
    "    )\n",
    "    print(\"Model reply:\\n\", reply)\n",
    "except Exception as e:\n",
    "    print(\"Call failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c9a57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
